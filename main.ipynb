{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ A FILE TEXT - EXTRACT AND PRINT THE PROPERTIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21303\n",
      "3798\n",
      "\n",
      "\n",
      "\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was \n"
     ]
    }
   ],
   "source": [
    "with open('new.txt', 'r', encoding=\"utf-8\") as f:\n",
    "\n",
    "    raw_text = f.read()\n",
    "    # TOTAL NUMBER OF CHARACTERS\n",
    "    print(len(raw_text))\n",
    "    # TOTAL NUMBER OF WORDS\n",
    "    print(len(raw_text.split()))\n",
    "    print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLITTING BASED ON WHITESPACE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# REMOVE WHITE SPACE\n",
    "\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4896\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\\\\cb1', '\\\\', '\\\\cb3', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1163\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Carlo;', 25)\n",
      "('Chicago', 26)\n",
      "('Claude', 27)\n",
      "('Come', 28)\n",
      "('Croft', 29)\n",
      "('Destroyed', 30)\n",
      "('Devonshire', 31)\n",
      "('Don', 32)\n",
      "('Dubarry', 33)\n",
      "('Emperors', 34)\n",
      "('Florence', 35)\n",
      "('For', 36)\n",
      "('Gallery', 37)\n",
      "('Gideon', 38)\n",
      "('Gisburn', 39)\n",
      "('Gisburns', 40)\n",
      "('Grafton', 41)\n",
      "('Greek', 42)\n",
      "('Grindle', 43)\n",
      "('Grindle:', 44)\n",
      "('Grindles', 45)\n",
      "('HAD', 46)\n",
      "('Had', 47)\n",
      "('Hang', 48)\n",
      "('Has', 49)\n",
      "('He', 50)\n",
      "('Her', 51)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SimpleTextTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab  # A\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}  # B\n",
    "        self.unk_token = '<UNK>' # UNK - UNKNOWN WORDS TO AVOID ERRORS\n",
    "        if self.unk_token not in self.str_to_int:\n",
    "            self.str_to_int[self.unk_token] = len(self.str_to_int) + 1\n",
    "            self.int_to_str[self.str_to_int[self.unk_token]] = self.unk_token\n",
    "\n",
    "    def encode(self, text):  # C\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int.get(s, self.str_to_int[self.unk_token]) for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):  # D\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)  # E\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 2, 875, 1016, 618, 544, 766, 5, 1158, 611, 5, 69, 7, 39, 876, 1139, 1164]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTextTokenizerV1(vocab)\n",
    "text = \"It's the last he painted, you know, Mrs. Gisburn said with pard\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It' s the last he painted, you know, Mrs. Gisburn said with <UNK>\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.decode(ids)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1165\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('your', 1160)\n",
      "('yourself', 1161)\n",
      "('}', 1162)\n",
      "('<|endoftext|>', 1163)\n",
      "('<|unk|>', 1164)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKENIZER TO HANDLE UNKNOWN WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SimpleTextTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab  # A\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}  # B\n",
    "        self.unk_token = '<UNK>' # UNK - UNKNOWN WORDS TO AVOID ERRORS\n",
    "        if self.unk_token not in self.str_to_int:\n",
    "            self.str_to_int[self.unk_token] = len(self.str_to_int) + 1\n",
    "            self.int_to_str[self.str_to_int[self.unk_token]] = self.unk_token\n",
    "\n",
    "    def encode(self, text):  # C\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int.get(s, self.str_to_int[self.unk_token]) for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):  # D\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)  # E\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1166, 5, 365, 1158, 645, 1003, 10, 1163, 57, 1016, 984, 1012, 741, 1016, 1166, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTextTokenizerV1(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK>, do you like tea? <|endoftext|> In the sunlit terraces of the <UNK>.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.5.1\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 3870]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of som\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of som\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5640\n"
     ]
    }
   ],
   "source": [
    "with open(\"NEW.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [27075, 11, 290, 4920]\n",
      "y:      [11, 290, 4920, 2241]\n"
     ]
    }
   ],
   "source": [
    "# PROCESSING INPUT TEXT WITH OUTPUT TEXT\n",
    "context_size = 4 #A\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27075] ----> 11\n",
      "[27075, 11] ----> 290\n",
      "[27075, 11, 290] ----> 4920\n",
      "[27075, 11, 290, 4920] ----> 2241\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " widow ----> ,\n",
      " widow, ---->  and\n",
      " widow, and ---->  established\n",
      " widow, and established ---->  himself\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt) #A\n",
    "        for i in range(0, len(token_ids) - max_length, stride): #B\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "def __len__(self): #C\n",
    "        return len(self.input_ids)\n",
    "def __getitem__(self, idx): #D\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4,\n",
    "    max_length=256, stride=128, shuffle=True, drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") #A\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #B\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4,\n",
    "        max_length=256, stride=128, shuffle=True, drop_last=True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") #A\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #B\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt) #A\n",
    "        for i in range(0, len(token_ids) - max_length, stride): #B\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    def __len__(self): #C\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx): #D\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUR TEXT ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[628, 198,  40, 367]]), tensor([[ 198,   40,  367, 2885]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"new.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader) #A\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.3035,  ..., -0.5850,  0.8768,  1.6221],\n",
      "        [-1.4779,  1.1331, -1.2203,  ..., -1.3193, -2.0915,  0.9629],\n",
      "        [-0.0319, -0.4790,  0.7668,  ...,  0.9224,  0.2056, -0.4970],\n",
      "        ...,\n",
      "        [ 0.9662, -0.7323,  1.0804,  ..., -0.1578, -0.1673, -0.9981],\n",
      "        [-0.2805,  0.8274, -0.3299,  ...,  0.9435,  1.0918,  1.0149],\n",
      "        [-1.0725, -0.1296, -0.3116,  ..., -1.7791,  0.9917,  0.0354]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(123)\n",
    "\n",
    "vocab_size = 100  \n",
    "output_dim = 50\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "print(embedding_layer.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(50257, 256)\n"
     ]
    }
   ],
   "source": [
    "# OUTPUT DIM - EMBEDDING VECTOR SIZE\n",
    "# VOCAB SIZE - TOTAL NUMBER OF WORDS\n",
    "output_dim = 256\n",
    "vocab_size = 50257\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(token_embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDS tensor([[  628,   198,    40,   367],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  257,  7026, 15632,   438],\n",
      "        [ 2016,   257,   922,  5891],\n",
      "        [ 1576,   438,   568,   340],\n",
      "        [  373,   645,  1049,  5975],\n",
      "        [  284,   502,   284,  3285]])\n",
      "TARGET IDS tensor([[  198,    40,   367,  2885],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [  271, 10899,  2138,   257],\n",
      "        [ 7026, 15632,   438,  2016],\n",
      "        [  257,   922,  5891,  1576],\n",
      "        [  438,   568,   340,   373],\n",
      "        [  645,  1049,  5975,   284],\n",
      "        [  502,   284,  3285,   326]])\n",
      "INPUT SHAPE torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False )\n",
    "data_iter = iter(dataloader)\n",
    "inputs , targets = next(data_iter)\n",
    "print(\"Token IDS\",inputs)\n",
    "print(\"TARGET IDS\",targets)\n",
    "print(\"INPUT SHAPE\",inputs.shape)\n",
    "\n",
    "# INPUT SHAPE FIRST NUMBER FROM BATCH SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING SHAPE torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"EMBEDDING SHAPE\", token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4300, 0.1500, 0.8910],\n",
      "        [0.5500, 0.8700, 0.6610],\n",
      "        [0.5700, 0.8500, 0.6410],\n",
      "        [0.2200, 0.5800, 0.3310],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.891],  # Your (x^1)\n",
    "    [0.55, 0.87, 0.661],  # journey (x^2)\n",
    "    [0.57, 0.85, 0.641],  # starts (x^3)\n",
    "    [0.22, 0.58, 0.331],  # with (x^4)\n",
    "    [0.77, 0.25, 0.10],   # one (x^5)\n",
    "    [0.05, 0.80, 0.55]    # step (x^6)\n",
    "])\n",
    "\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9560, 1.4963, 1.4767, 0.8444, 0.7071, 1.0871])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  #A\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1456, 0.2278, 0.2248, 0.1286, 0.1077, 0.1655])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1386, 0.2380, 0.2333, 0.1240, 0.1081, 0.1580])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    " \n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTEXT VECTOR \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5691])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2_naive[i]*x_i\n",
    "print(context_vec_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 μs, sys: 1 μs, total: 2 μs\n",
      "Wall time: 3.1 μs\n",
      "tensor([[1.0013, 0.9560, 0.9437, 0.4765, 0.4577, 0.6316],\n",
      "        [0.9560, 1.4963, 1.4767, 0.8444, 0.7071, 1.0871],\n",
      "        [0.9437, 1.4767, 1.4583, 0.8306, 0.7155, 1.0610],\n",
      "        [0.4765, 0.8444, 0.8306, 0.4944, 0.3475, 0.6571],\n",
      "        [0.4577, 0.7071, 0.7155, 0.3475, 0.6654, 0.2935],\n",
      "        [0.6316, 1.0871, 1.0610, 0.6571, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 μs, sys: 0 ns, total: 1 μs\n",
      "Wall time: 2.15 μs\n",
      "tensor([[1.0013, 0.9560, 0.9437, 0.4765, 0.4577, 0.6316],\n",
      "        [0.9560, 1.4963, 1.4767, 0.8444, 0.7071, 1.0871],\n",
      "        [0.9437, 1.4767, 1.4583, 0.8306, 0.7155, 1.0611],\n",
      "        [0.4765, 0.8444, 0.8306, 0.4944, 0.3475, 0.6571],\n",
      "        [0.4577, 0.7071, 0.7155, 0.3475, 0.6654, 0.2935],\n",
      "        [0.6316, 1.0871, 1.0611, 0.6571, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2099, 0.2006, 0.1982, 0.1242, 0.1219, 0.1451],\n",
      "        [0.1386, 0.2380, 0.2333, 0.1240, 0.1081, 0.1580],\n",
      "        [0.1391, 0.2370, 0.2327, 0.1242, 0.1107, 0.1564],\n",
      "        [0.1436, 0.2074, 0.2046, 0.1462, 0.1262, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1878, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1421, 0.0988, 0.1895]])\n",
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "print(attn_weights)\n",
    "\n",
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5798],\n",
      "        [0.4419, 0.6515, 0.5691],\n",
      "        [0.4431, 0.6496, 0.5679],\n",
      "        [0.4304, 0.6298, 0.5518],\n",
      "        [0.4671, 0.5910, 0.5273],\n",
      "        [0.4177, 0.6503, 0.5653]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] #A\n",
    "d_in = inputs.shape[1] #B\n",
    "d_out = 2 #C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4307, 1.4559])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8545)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1] #A\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2723, 1.8545, 1.8132, 1.0813, 0.5580, 1.5448])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2265, 0.2199, 0.1311, 0.0905, 0.1819])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8545)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1] #A\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.16 After computing the attention scores ω, the next step is to normalize these scores using the softmax function to obtain the attention weights α."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2723, 1.8545, 1.8132, 1.0813, 0.5580, 1.5448])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3062, 0.8217])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTENTION MECHANISM CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2997, 0.8060],\n",
      "        [0.3062, 0.8217],\n",
      "        [0.3059, 0.8210],\n",
      "        [0.2949, 0.7945],\n",
      "        [0.2928, 0.7897],\n",
      "        [0.2991, 0.8047]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A self-attention class using PyTorch's Linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5,dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0740,  0.0712],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0750,  0.0701],\n",
      "        [-0.0761,  0.0683],\n",
      "        [-0.0765,  0.0678],\n",
      "        [-0.0756,  0.0692]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MASKED ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1922, 0.1647, 0.1652, 0.1550, 0.1721, 0.1509],\n",
      "        [0.2042, 0.1659, 0.1662, 0.1496, 0.1664, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1663, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1831, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1665, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)  #A\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2042, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1663, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1831, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1665, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2759, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1970, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1665, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2904,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4663, 0.1727,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4601, 0.1708, 0.1735,   -inf,   -inf,   -inf],\n",
      "        [0.2646, 0.1027, 0.1039, 0.0189,   -inf,   -inf],\n",
      "        [0.2185, 0.0876, 0.0884, 0.0179, 0.0786,   -inf],\n",
      "        [0.3411, 0.1274, 0.1294, 0.0201, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2759, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1970, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1665, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #A\n",
    "example = torch.ones(6, 6) #B\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) #A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A compact causal attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super(CausalAttention, self).__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
    "        \n",
    "        # Register a buffer for the mask\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape  # Batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        \n",
    "        # Apply mask to the attention scores\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask[:num_tokens, :num_tokens] == 1, float('-inf')\n",
    "        )\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Compute the context vector\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, num_heads, qkv_bias=False):\n",
    "        super(MultiHeadAttentionWrapper, self).__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4774, 0.1063, 0.4568, 0.2731],\n",
      "         [0.5893, 0.3256, 0.5794, 0.3013],\n",
      "         [0.6205, 0.3859, 0.6251, 0.3104],\n",
      "         [0.5480, 0.3589, 0.5693, 0.2788],\n",
      "         [0.5323, 0.3427, 0.5545, 0.2521],\n",
      "         [0.5078, 0.3493, 0.5339, 0.2501]],\n",
      "\n",
      "        [[0.4774, 0.1063, 0.4568, 0.2731],\n",
      "         [0.5893, 0.3256, 0.5794, 0.3013],\n",
      "         [0.6205, 0.3859, 0.6251, 0.3104],\n",
      "         [0.5480, 0.3589, 0.5693, 0.2788],\n",
      "         [0.5323, 0.3427, 0.5545, 0.2521],\n",
      "         [0.5078, 0.3493, 0.5339, 0.2501]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "context_length = batch.shape[1]  # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "num_heads = 2  # Example number of heads\n",
    "\n",
    "# Instantiate the MultiHeadAttentionWrapper class\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads)\n",
    "\n",
    "# Get the context vectors\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "# Print the context vectors and their shape\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An efficient multi-head attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1).bool())\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(-2, -1) / self.head_dim**0.5\n",
    "\n",
    "        mask_bool = self.mask[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, float('-inf'))\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2).contiguous()\n",
    "        context_vec = context_vec.view(b, num_tokens, self.d_out)\n",
    "\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573], #A\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "print(a @ a.transpose(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 1024,      # Context length\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "# Embedding dimension\n",
    "# Number of attention heads\n",
    "# Number of layers\n",
    "# Dropout rate\n",
    "# Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A placeholder GPT model architecture class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # Define the layers within the transformer block here\n",
    "        # For example, self.attention = nn.MultiheadAttention(...)\n",
    "        # self.feed_forward = nn.Sequential(...)\n",
    "        pass  # Replace with actual implementation\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        \n",
    "        self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "    class DummyTransformerBlock(nn.Module): #C\n",
    "        def __init__(self, cfg):\n",
    "            super().__init__()\n",
    "        def forward(self, x): #D\n",
    "            return x\n",
    "    class DummyLayerNorm(nn.Module): #E\n",
    "        def __init__(self, normalized_shape, eps=1e-5): #F\n",
    "            super().__init__()\n",
    "        def forward(self, x):\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGAIN REDOING CODE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configuration for a dummy GPT model\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_layers\": 12,\n",
    "    \"n_heads\": 12,\n",
    "    \"drop_rate\": 0.1\n",
    "}\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(DummyTransformerBlock, self).__init__()\n",
    "        self.ln1 = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.attn = nn.MultiheadAttention(cfg[\"emb_dim\"], cfg[\"n_heads\"], dropout=cfg[\"drop_rate\"])\n",
    "        self.ln2 = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "            nn.Dropout(cfg[\"drop_rate\"]),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        x = x + attn_output\n",
    "        x = self.ln1(x)\n",
    "        ff_output = self.ff(x)\n",
    "        x = x + ff_output\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(DummyGPTModel, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.token_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        \n",
    "        self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.shape\n",
    "        pos = torch.arange(seq_length, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        token_emb = self.token_emb(x)\n",
    "        pos_emb = self.pos_emb(pos)\n",
    "        x = self.drop_emb(token_emb + pos_emb)\n",
    "        \n",
    "        x = x.permute(1, 0, 2)  # (seq_length, batch_size, emb_dim)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = x.permute(1, 0, 2)  # (batch_size, seq_length, emb_dim)\n",
    "        \n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 20, 50257])\n",
      "tensor([[[-1.3599, -1.2577, -0.6111,  ...,  0.0061,  0.0420, -1.0768],\n",
      "         [-0.6184, -0.3793,  0.2606,  ..., -1.4515,  0.1570, -0.7658],\n",
      "         [ 0.6675, -0.6125, -0.3853,  ..., -0.4462, -0.7583, -1.1308],\n",
      "         ...,\n",
      "         [-0.2660,  0.1238,  0.5709,  ..., -0.5192, -0.2916,  0.6920],\n",
      "         [-0.9846, -0.1891, -0.2625,  ..., -0.0740, -0.4961, -0.1376],\n",
      "         [-0.9344,  0.0295, -0.3976,  ..., -0.3331, -1.1134, -0.5943]],\n",
      "\n",
      "        [[-0.7966, -0.4925,  0.2969,  ..., -0.6393, -0.1343, -1.1402],\n",
      "         [-0.2599, -0.1856,  0.0412,  ...,  0.1443,  0.6179, -0.7266],\n",
      "         [ 0.1892, -0.3393, -0.1118,  ..., -0.8784, -0.5399, -0.3858],\n",
      "         ...,\n",
      "         [-0.5847, -0.4795,  0.4931,  ..., -0.5003,  0.5226, -0.2928],\n",
      "         [-0.4816, -0.0598, -1.0979,  ..., -0.3643, -0.6581, -0.0875],\n",
      "         [ 0.0280,  0.2219, -0.5682,  ..., -0.3100,  0.1207, -0.4615]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# IMPORTING 124 MILLION PARAMETERS MODEL\n",
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "batch = torch.randint(0, GPT_CONFIG_124M[\"vocab_size\"], (2, 20))  # (batch_size, seq_length)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# layer normalization\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5) #A\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[-5.9605e-08],\n",
      "        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASS LAYER NORMALIZATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[-2.9802e-08],\n",
      "        [ 0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# LayerNorm module in practice and apply it to the batch input:\n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOTING THE FUNCTIONS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "x = torch.linspace(-3, 3, 100) #A\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A feed forward neural network module\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    ")\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "# FEED FORWARD NETWORK\n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768) #A\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN WITH SHORTCUT CONNECTION\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            # Implement 5 layers\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n",
    "])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) # specify random seed for the initial weights for re\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function that computes the gradients in the the model's backward pass:\n",
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152039906941354\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)\n",
    "# THE DECREASING IN GRADIENTS CALLED VANISHING GRADIENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "# MODEL WITH SKIP CONNECTIONS\n",
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, block_size, num_heads, dropout, qkv_bias):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=d_in, num_heads=num_heads, dropout=dropout, bias=qkv_bias)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0, 2)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output.permute(1, 0, 2)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"] * 4)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"] * 4, cfg[\"emb_dim\"])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "    def forward(self, x):\n",
    "        return self.norm(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            block_size=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(GPTModel, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.token_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(12)])  # Using 12 layers as an example\n",
    "        self.ln_f = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_proj = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        self.out_proj.weight = self.token_emb.weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        token_embeddings = self.token_emb(x)\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "        position_embeddings = self.pos_emb(position_ids)\n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.drop_emb(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.out_proj(x)\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, idx, max_length, do_sample=True, top_k=50, temperature=0.7):\n",
    "        self.eval()  # Ensure model is in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            outputs = []\n",
    "            current_token_ids = idx.clone()\n",
    "            for _ in range(max_length):\n",
    "                logits = self.forward(current_token_ids)\n",
    "                if do_sample:\n",
    "                    logits = logits[:, -1, :] / temperature\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)\n",
    "                else:\n",
    "                    next_token = torch.topk(logits[:, -1, :], k=top_k, dim=-1)\n",
    "                    next_token = next_token.squeeze(1)\n",
    "\n",
    "                outputs.append(next_token.item())\n",
    "                # Ensure next_token is correctly shaped for concatenation\n",
    "                next_token = next_token.unsqueeze(0)  # Add batch dimension\n",
    "                current_token_ids = torch.cat([current_token_ids, next_token], dim=1)\n",
    "        return current_token_ids\n",
    "\n",
    "\n",
    "\n",
    "# Define the GPT configuration with the missing key included\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)  # Example input tensor\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4.5 Connecting attention and linear layers in a transformer block\n",
    "\n",
    "# GPT_CONFIG_124M = {\n",
    "#     \"emb_dim\": 768,\n",
    "#     \"context_length\": 4,\n",
    "#     \"n_heads\": 12,\n",
    "#     \"drop_rate\": 0.1,\n",
    "#     \"qkv_bias\": True,  # Ensure this key is included\n",
    "#     # Include other necessary keys for the FeedForward module if any\n",
    "# }\n",
    "\n",
    "# class TransformerBlock(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.att = MultiHeadAttention(\n",
    "#             d_in=cfg[\"emb_dim\"],\n",
    "#             d_out=cfg[\"emb_dim\"],\n",
    "#             block_size=cfg[\"context_length\"],\n",
    "#             num_heads=cfg[\"n_heads\"],\n",
    "#             dropout=cfg[\"drop_rate\"],\n",
    "#             qkv_bias=cfg[\"qkv_bias\"])\n",
    "#         self.ff = FeedForward(cfg)\n",
    "#         self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "#         self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "#         self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "#     def forward(self, x):\n",
    "#         #A\n",
    "#         shortcut = x\n",
    "#         x = self.norm1(x)\n",
    "#         x = self.att(x)\n",
    "#         x = self.drop_resid(x)\n",
    "#         x = x + shortcut  # Add the original input back\n",
    "#         shortcut = x #B\n",
    "#         x = self.norm2(x)\n",
    "#         x = self.ff(x)\n",
    "#         x = self.drop_resid(x)\n",
    "#         x = x + shortcut  #C\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)  # Example input tensor\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GPT model architecture implementation\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[25371, 42188, 47556,  8856, 13224, 29264, 30628,  7204, 34725, 39523,\n",
      "         44813,  1115, 28954, 44349, 34500,  8399,  9783, 15353, 30529, 22184],\n",
      "        [16499, 30185,  1259, 35534, 46286, 38674, 39569, 36623, 18047,  6669,\n",
      "         13955, 46877, 48518, 48812, 34325, 42640, 42819, 34347, 27687, 50069]])\n",
      "\n",
      "Output shape: torch.Size([2, 20, 50257])\n",
      "tensor([[[-1.0871, -0.2160, -0.1754,  ..., -0.2219, -0.8070, -0.4957],\n",
      "         [-0.7125,  0.2712, -0.5051,  ..., -0.0540, -0.8916, -0.0825],\n",
      "         [-0.1383,  0.0270, -0.1756,  ...,  0.1413,  0.3794, -0.1628],\n",
      "         ...,\n",
      "         [-1.5354,  0.6456,  1.1948,  ...,  0.9305, -0.2489,  0.8868],\n",
      "         [ 0.3126,  0.0398, -0.0674,  ..., -0.6247,  0.0696,  1.4717],\n",
      "         [-0.5091,  0.8648, -0.8332,  ...,  0.1936,  0.3052,  0.2909]],\n",
      "\n",
      "        [[-1.8764,  0.0362, -0.4020,  ...,  0.6799,  0.0400, -0.4855],\n",
      "         [-1.1981,  0.3524, -0.9005,  ..., -0.4203, -0.2029,  0.8310],\n",
      "         [-0.2131,  0.0675, -0.5268,  ..., -0.1984,  1.2677,  0.2929],\n",
      "         ...,\n",
      "         [ 0.1487,  0.1355,  0.6606,  ..., -0.5238, -0.4025, -0.4064],\n",
      "         [ 0.0679,  0.4127, -0.5517,  ..., -0.4767,  0.4748, -0.0130],\n",
      "         [-1.1970,  0.0287, -0.7848,  ...,  0.1765,  0.7842,  0.1762]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,000,320\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,000,320\n",
      "Total number of parameters: 163,000,320\n",
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "# The token embedding and output layers are very large due to the number of rows for the 50,257 in the\n",
    "#  tokenizer's vocabulary. Let's remove the output layer parameter count from the total GPT-2 model \n",
    "# count according to the weight tying:\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "# Since weights are tied, we count the token embedding parameters only once\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.80 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4  #A\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)  #B\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size): #A\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:] #B\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :] #C\n",
    "        probas = torch.softmax(logits, dim=-1)  #D\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True) #E\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  #F\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "#  TEXT GENERATING\n",
    "\n",
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716,  6464,  2299,  4695,  2583, 16896, 29091]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "# EVALUATING THE GENERATED TEXT\n",
    "model.eval() #A\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am receiving compl animals host cried vitamins\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm(\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,  #A\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,     #B\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Clock aligned cess predators internationally Fisheries piles someone precipitationbsp\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimensi\n",
    "    return encoded_tensor\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets are the inputs but shifted one position forward, a concept\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [588,  428,  11311]]) #  \" really like chocolate\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "# feed the inputs into the model to calculate logit vectors for the two input examples, \n",
    "# each comprising three tokens, and apply the softmax function to transform these logit values into probability scores\n",
    "with torch.no_grad(): #A\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocab\n",
    "print(probas.shape)\n",
    "\n",
    "# OUTPUT - FIRST : 2 EXAMPLE IN THE INPUT BATCH , 2 : 3 NUMBER OF TOKENS IN EACH EXAMPLE, 3 : 50257 NUMBER OF TOKENS IN VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[ 8102],\n",
      "         [  351],\n",
      "         [39178]],\n",
      "\n",
      "        [[19646],\n",
      "         [16501],\n",
      "         [38197]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1: abis with loopholes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([8.1353e-06, 6.1600e-05, 3.7829e-05])\n",
      "Text 2: tensor([1.1551e-05, 7.0061e-06, 2.9883e-05])\n"
     ]
    }
   ],
   "source": [
    "# For each of the two input texts, we can print the initial softmax probability scores corresponding \n",
    "# to the target tokens via the following code:\n",
    "\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    " \n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-11.7193,  -9.6948, -10.1824, -11.3688, -11.8687, -10.4182])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.8754)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8754)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8754)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8754)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8754)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(52858.9062)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"new.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 21303\n",
      "Tokens: 5640\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# we can iterate through the data loaders to ensure that they were created correctly:\n",
    "\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY FUNCTION TO CALCULATE CROSS ENTROPY FUNCTION FOR THE MODEL\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO COMPUTE THE TRAINING AND VALIDATION LOSS\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader) #A\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader)) #B\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item() #C\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches #D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.954476144578722\n",
      "Validation loss: 10.97632122039795\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #A\n",
    "model.to(device)\n",
    "train_loss = calc_loss_loader(train_loader, model, device) #B\n",
    "val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device,\n",
    "                       eval_freq, eval_iter, start_context, num_epochs):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], [] #A\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    for epoch in range(num_epochs): #B\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() #C\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() #D\n",
    "            optimizer.step() #E\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "            if global_step % eval_freq == 0: #F\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        generate_and_print_sample(  #G\n",
    "            model, train_loader.dataset.tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval() #A\n",
    "    with torch.no_grad(): #B\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=4)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.189, Val loss 9.860\n",
      "Ep 1 (Step 000005): Train loss 7.810, Val loss 8.172\n",
      "Every effort moves you the,,,, the, the,,,,,,, the,, the,,, the,,, the,,,,,,,,,,,,,,,, the,,,,,,\n",
      "Ep 2 (Step 000010): Train loss 6.256, Val loss 6.810\n",
      "Ep 2 (Step 000015): Train loss 5.794, Val loss 6.319\n",
      "Every effort moves you I of the, I of the I.\\cb1 \\ \\cb1 \\ \\cb1 \\ \\cb1 \\ \\cb1 \\ \\cb1 \\ \\cb1 \\ \\cb1 \\ \\\n",
      "Ep 3 (Step 000020): Train loss 5.378, Val loss 6.158\n",
      "Ep 3 (Step 000025): Train loss 5.409, Val loss 6.021\n",
      "Every effort moves you3 \"I, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and the, and of the, and\n",
      "Ep 4 (Step 000030): Train loss 4.579, Val loss 6.016\n",
      "Ep 4 (Step 000035): Train loss 4.023, Val loss 5.956\n",
      "Every effort moves you of the \"I I had been his pictures of the \"I of the of the, and I was of the \\ \\cb1 \\ \\cb1 \\ \\cb1 \\ \\cb1 \\ \\cb1 \\ \n",
      "Ep 5 (Step 000040): Train loss 4.321, Val loss 5.873\n",
      "Every effort moves you know of Jack's I had been his pictures--the--the to have been his own--the, as a little to have been to have been his own the, and he had been. It was a little was a little was his own he\n",
      "Ep 6 (Step 000045): Train loss 3.069, Val loss 5.846\n",
      "Ep 6 (Step 000050): Train loss 3.539, Val loss 5.829\n",
      "Every effort moves you know the picture--and me to have to the picture--the to me--and, and it was no of the to me to the picture--and that he had been he was dead?, and he had been the picture to the last he\n",
      "Ep 7 (Step 000055): Train loss 1.707, Val loss 5.877\n",
      "Ep 7 (Step 000060): Train loss 1.919, Val loss 5.906\n",
      "Every effort moves you know the picture of the picture to have to the picture--his to see his pictures--as he was no great surprise to see it to my eyes his own he had been he was dead? You?\"\\cb3 \"I didn't bear the\n",
      "Ep 8 (Step 000065): Train loss 1.506, Val loss 5.925\n",
      "Ep 8 (Step 000070): Train loss 0.819, Val loss 5.962\n",
      "Every effort moves you'd tell me how it happened,\" she said.\\cb1 \\ \\cb3 He stood looking up at the sketch, and twirling between his fingers a cigarette he had been,\" she said, and he's not looking,\" she said,\n",
      "Ep 9 (Step 000075): Train loss 0.945, Val loss 6.048\n",
      "Ep 9 (Step 000080): Train loss 0.645, Val loss 6.091\n",
      "Every effort moves you'd tell me how it happened,\" I had always been through the prism of my bravura tricks. Gisburn's head between his knees. Gisburn had given up the drawing-room, the end of the florid vista.\n",
      "Ep 10 (Step 000085): Train loss 0.454, Val loss 6.132\n",
      "Every effort moves you'd tell me how it happened,\" I said. I had only to set up the easel and get to work. I had been dead only twenty-four hours, I had not existed till nearly a year after Jack's resolve had been no preliminary\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=1,\n",
    "    start_context=\"Every effort moves you\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")  # Fix: Proper indentation\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax2 = ax1.twiny()  #A\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  #B\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm(\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you with boyfriend I found the couple at tea beneath their palm-trees; and Mrs. Gisburn's welcome was so genial\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you with boyfriend\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "# we convert the logits into probabilities via the softmax function and obtain the token ID corresponding the generated token via the \n",
    "# argmax function, which we can then map back into text via the inverse vocabulary:\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "5 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "5 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(10)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see based on the output, the word \"forward\" is sampled most of the time \n",
    "# (582 out of 1000 times), but other tokens such as \"closer\", \"inches\", and \"toward\" will also be sampled\n",
    "#  some of the time. This means that if we replaced the argmax function with the multinomial function inside\n",
    "#  the generate_and_print_sample function, the LLM would sometimes generatetextssuchas\"every effort moves you toward\",\n",
    "# \"every effort moves you inches\",\n",
    "# and\"every effort moves you closer\"insteadof \"every effort moves you forward\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperatures = [1, 0.1, 5]  # Original, higher, and lower temperature\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i],\n",
    "                   bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    " \n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],  #A\n",
    "    input=torch.tensor(float('-inf')),  #B\n",
    "    other=next_token_logits  #C\n",
    ")\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.3 Modifying the text generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you an object for garlands ever knew.\" to have dis! The women had\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Loading and saving model weights in PyTorch\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm(\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm(\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "# We can then download the model weights for the 124 million parameter model as follows:\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm(\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: gpt2-small (124M)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2-small (124M)\"\n",
    "print(f\"Model name: {model_name}\")\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"context_length\": 1024})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"qkv_bias\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm(\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO CHECK LEFT AND RIGHT TENSOR DIAMENSIONS ARE SME\n",
    "# def assign(left, right):\n",
    "#     if left.shape != right.shape:\n",
    "#         raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right}\")\n",
    "#     return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right).clone().detach()) # Changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_requirements_dict():\n",
    "    return {\n",
    "        'numpy': '1.26.0',\n",
    "        'matplotlib': '3.8.2',\n",
    "        'jupyterlab': '4.0.6',\n",
    "        'tensorflow': '2.15.0',\n",
    "        'torch': '2.2.1',\n",
    "        'tqdm': '4.66.1',\n",
    "        'tiktoken': '0.5.1'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required packages are already installed.\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "def verify_installed_packages(requirements_dict):\n",
    "    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "    missing_packages = []\n",
    "\n",
    "    for package, required_version in requirements_dict.items():\n",
    "        if package not in installed_packages:\n",
    "            missing_packages.append(package)\n",
    "    \n",
    "    if missing_packages:\n",
    "        print(\"The following packages need to be installed:\")\n",
    "        for package in missing_packages:\n",
    "            print(package)\n",
    "    else:\n",
    "        print(\"All required packages are already installed.\")\n",
    "\n",
    "# Main script execution\n",
    "if __name__ == \"__main__\":\n",
    "    d = get_requirements_dict()\n",
    "    verify_installed_packages(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256,  # Adjusted context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (relu): ReLU()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (norm1): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm2): LayerNorm(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm(\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "# NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[200], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[1;32m     90\u001b[0m gpt \u001b[38;5;241m=\u001b[39m GPTModel(GPT_CONFIG_124M)\n\u001b[0;32m---> 91\u001b[0m load_weights_into_gpt(gpt, params)\n\u001b[1;32m     92\u001b[0m gpt\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[200], line 22\u001b[0m, in \u001b[0;36mload_weights_into_gpt\u001b[0;34m(gpt, params)\u001b[0m\n\u001b[1;32m     16\u001b[0m gpt\u001b[38;5;241m.\u001b[39mtrf_blocks[b]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39min_proj_weight \u001b[38;5;241m=\u001b[39m assign(\n\u001b[1;32m     17\u001b[0m     gpt\u001b[38;5;241m.\u001b[39mtrf_blocks[b]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39min_proj_weight, \n\u001b[1;32m     18\u001b[0m     np\u001b[38;5;241m.\u001b[39mconcatenate([q_w\u001b[38;5;241m.\u001b[39mT, k_w\u001b[38;5;241m.\u001b[39mT, v_w\u001b[38;5;241m.\u001b[39mT], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     20\u001b[0m q_b, k_b, v_b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msplit(\n\u001b[1;32m     21\u001b[0m     (params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks\u001b[39m\u001b[38;5;124m\"\u001b[39m][b][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m3\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m gpt\u001b[38;5;241m.\u001b[39mtrf_blocks[b]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39min_proj_bias \u001b[38;5;241m=\u001b[39m assign(\n\u001b[1;32m     23\u001b[0m     gpt\u001b[38;5;241m.\u001b[39mtrf_blocks[b]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39min_proj_bias, \n\u001b[1;32m     24\u001b[0m     np\u001b[38;5;241m.\u001b[39mconcatenate([q_b, k_b, v_b], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     26\u001b[0m gpt\u001b[38;5;241m.\u001b[39mtrf_blocks[b]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m assign(\n\u001b[1;32m     27\u001b[0m     gpt\u001b[38;5;241m.\u001b[39mtrf_blocks[b]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight, \n\u001b[1;32m     28\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks\u001b[39m\u001b[38;5;124m\"\u001b[39m][b][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m     29\u001b[0m gpt\u001b[38;5;241m.\u001b[39mtrf_blocks[b]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m assign(\n\u001b[1;32m     30\u001b[0m     gpt\u001b[38;5;241m.\u001b[39mtrf_blocks[b]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias, \n\u001b[1;32m     31\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks\u001b[39m\u001b[38;5;124m\"\u001b[39m][b][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[200], line 6\u001b[0m, in \u001b[0;36massign\u001b[0;34m(old_weights, new_weights)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massign\u001b[39m(old_weights, new_weights):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Assuming old_weights is a PyTorch Parameter\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     old_weights\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(new_weights)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m old_weights\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def assign(old_weights, new_weights):\n",
    "    # Assuming old_weights is a PyTorch Parameter\n",
    "    old_weights.data = torch.from_numpy(new_weights)\n",
    "    return old_weights\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.attention.in_proj_weight = assign(\n",
    "            gpt.trf_blocks[b].att.attention.in_proj_weight, \n",
    "            np.concatenate([q_w.T, k_w.T, v_w.T], axis=0))\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.attention.in_proj_bias = assign(\n",
    "            gpt.trf_blocks[b].att.attention.in_proj_bias, \n",
    "            np.concatenate([q_b, k_b, v_b], axis=0))\n",
    "\n",
    "        gpt.trf_blocks[b].att.attention.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.attention.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.attention.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.attention.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.fc1.weight = assign(\n",
    "            gpt.trf_blocks[b].ff.fc1.weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.fc1.bias = assign(\n",
    "            gpt.trf_blocks[b].ff.fc1.bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.fc2.weight = assign(\n",
    "            gpt.trf_blocks[b].ff.fc2.weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.fc2.bias = assign(\n",
    "            gpt.trf_blocks[b].ff.fc2.bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.norm.weight = assign(\n",
    "            gpt.trf_blocks[b].norm1.norm.weight, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.norm.bias = assign(\n",
    "            gpt.trf_blocks[b].norm1.norm.bias, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.norm.weight = assign(\n",
    "            gpt.trf_blocks[b].norm2.norm.weight, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.norm.bias = assign(\n",
    "            gpt.trf_blocks[b].norm2.norm.bias, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.norm.weight = assign(gpt.final_norm.norm.weight, params[\"g\"])\n",
    "    gpt.final_norm.norm.bias = assign(gpt.final_norm.norm.bias, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n",
    "# Example parameters for testing\n",
    "params = {\n",
    "    \"wpe\": np.random.rand(GPT_CONFIG_124M[\"context_length\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
    "    \"wte\": np.random.rand(GPT_CONFIG_124M[\"vocab_size\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
    "    \"blocks\": [{\n",
    "        \"attn\": {\n",
    "            \"c_attn\": {\"w\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"] * 3),\n",
    "                       \"b\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"] * 3)},\n",
    "            \"c_proj\": {\"w\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
    "                       \"b\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"])}\n",
    "        },\n",
    "        \"mlp\": {\n",
    "            \"c_fc\": {\"w\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"] * 4, GPT_CONFIG_124M[\"emb_dim\"]),\n",
    "                     \"b\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"] * 4)},\n",
    "            \"c_proj\": {\"w\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"] * 4),\n",
    "                       \"b\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"])}\n",
    "        },\n",
    "        \"ln_1\": {\"g\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"]),\n",
    "                 \"b\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"])},\n",
    "        \"ln_2\": {\"g\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"]),\n",
    "                 \"b\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"])}\n",
    "    } for _ in range(GPT_CONFIG_124M[\"n_layers\"])],\n",
    "    \"g\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"]),\n",
    "    \"b\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"])\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "gpt = GPTModel(GPT_CONFIG_124M)\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[201], line 171\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[1;32m    170\u001b[0m gpt \u001b[38;5;241m=\u001b[39m GPTModel(GPT_CONFIG_124M)\n\u001b[0;32m--> 171\u001b[0m load_weights_into_gpt(gpt, params)\n\u001b[1;32m    172\u001b[0m gpt\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[201], line 102\u001b[0m, in \u001b[0;36mload_weights_into_gpt\u001b[0;34m(gpt, params)\u001b[0m\n\u001b[1;32m     96\u001b[0m gpt\u001b[38;5;241m.\u001b[39mtrf_blocks[b]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39min_proj_weight \u001b[38;5;241m=\u001b[39m assign(\n\u001b[1;32m     97\u001b[0m     gpt\u001b[38;5;241m.\u001b[39mtrf_blocks[b]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39min_proj_weight, \n\u001b[1;32m     98\u001b[0m     np\u001b[38;5;241m.\u001b[39mconcatenate([q_w\u001b[38;5;241m.\u001b[39mT, k_w\u001b[38;5;241m.\u001b[39mT, v_w\u001b[38;5;241m.\u001b[39mT], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    100\u001b[0m q_b, k_b, v_b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msplit(\n\u001b[1;32m    101\u001b[0m     (params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks\u001b[39m\u001b[38;5;124m\"\u001b[39m][b][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m3\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m gpt\u001b[38;5;241m.\u001b[39mtrf_blocks[b]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39min_proj_bias \u001b[38;5;241m=\u001b[39m assign(\n\u001b[1;32m    103\u001b[0m     gpt\u001b[38;5;241m.\u001b[39mtrf_blocks[b]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39min_proj_bias, \n\u001b[1;32m    104\u001b[0m     np\u001b[38;5;241m.\u001b[39mconcatenate([q_b, k_b, v_b], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    106\u001b[0m gpt\u001b[38;5;241m.\u001b[39mtrf_blocks[b]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m assign(\n\u001b[1;32m    107\u001b[0m     gpt\u001b[38;5;241m.\u001b[39mtrf_blocks[b]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight, \n\u001b[1;32m    108\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks\u001b[39m\u001b[38;5;124m\"\u001b[39m][b][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m    109\u001b[0m gpt\u001b[38;5;241m.\u001b[39mtrf_blocks[b]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m assign(\n\u001b[1;32m    110\u001b[0m     gpt\u001b[38;5;241m.\u001b[39mtrf_blocks[b]\u001b[38;5;241m.\u001b[39matt\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias, \n\u001b[1;32m    111\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks\u001b[39m\u001b[38;5;124m\"\u001b[39m][b][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[201], line 86\u001b[0m, in \u001b[0;36massign\u001b[0;34m(old_weights, new_weights)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massign\u001b[39m(old_weights, new_weights):\n\u001b[0;32m---> 86\u001b[0m     old_weights\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(new_weights)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m old_weights\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, block_size, num_heads, dropout, qkv_bias):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=d_in, num_heads=num_heads, dropout=dropout, bias=qkv_bias)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0, 2)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output.permute(1, 0, 2)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"] * 4)\n",
    "        self.fc2 = nn.Linear(cfg[\"emb_dim\"] * 4, cfg[\"emb_dim\"])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "    def forward(self, x):\n",
    "        return self.norm(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            block_size=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(GPTModel, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "def assign(old_weights, new_weights):\n",
    "    old_weights.data = torch.from_numpy(new_weights)\n",
    "    return old_weights\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.attention.in_proj_weight = assign(\n",
    "            gpt.trf_blocks[b].att.attention.in_proj_weight, \n",
    "            np.concatenate([q_w.T, k_w.T, v_w.T], axis=0))\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.attention.in_proj_bias = assign(\n",
    "            gpt.trf_blocks[b].att.attention.in_proj_bias, \n",
    "            np.concatenate([q_b, k_b, v_b], axis=0))\n",
    "\n",
    "        gpt.trf_blocks[b].att.attention.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.attention.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.attention.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.attention.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.fc1.weight = assign(\n",
    "            gpt.trf_blocks[b].ff.fc1.weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.fc1.bias = assign(\n",
    "            gpt.trf_blocks[b].ff.fc1.bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.fc2.weight = assign(\n",
    "            gpt.trf_blocks[b].ff.fc2.weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.fc2.bias = assign(\n",
    "            gpt.trf_blocks[b].ff.fc2.bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.norm.weight = assign(\n",
    "            gpt.trf_blocks[b].norm1.norm.weight, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.norm.bias = assign(\n",
    "            gpt.trf_blocks[b].norm1.norm.bias, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.norm.weight = assign(\n",
    "            gpt.trf_blocks[b].norm2.norm.weight, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.norm.bias = assign(\n",
    "            gpt.trf_blocks[b].norm2.norm.bias, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.norm.weight = assign(gpt.final_norm.norm.weight, params[\"g\"])\n",
    "    gpt.final_norm.norm.bias = assign(gpt.final_norm.norm.bias, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n",
    "# Example parameters for testing\n",
    "params = {\n",
    "    \"wpe\": np.random.rand(GPT_CONFIG_124M[\"context_length\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
    "    \"wte\": np.random.rand(GPT_CONFIG_124M[\"vocab_size\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
    "    \"blocks\": [{\n",
    "        \"attn\": {\n",
    "            \"c_attn\": {\"w\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"] * 3),\n",
    "                       \"b\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"] * 3)},\n",
    "            \"c_proj\": {\"w\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"]),\n",
    "                       \"b\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"])}\n",
    "        },\n",
    "        \"mlp\": {\n",
    "            \"c_fc\": {\"w\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"] * 4, GPT_CONFIG_124M[\"emb_dim\"]),\n",
    "                     \"b\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"] * 4)},\n",
    "            \"c_proj\": {\"w\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"emb_dim\"] * 4),\n",
    "                       \"b\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"])}\n",
    "        },\n",
    "        \"ln_1\": {\"g\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"]),\n",
    "                 \"b\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"])},\n",
    "        \"ln_2\": {\"g\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"]),\n",
    "                 \"b\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"])}\n",
    "    } for _ in range(GPT_CONFIG_124M[\"n_layers\"])],\n",
    "    \"g\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"]),\n",
    "    \"b\": np.random.rand(GPT_CONFIG_124M[\"emb_dim\"])\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "gpt = GPTModel(GPT_CONFIG_124M)\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort will seChinese skate PatientcomfortLeg Diseases� sermonс hyster Lower758 mechanical deliversсiners pa neoliberal fishesс unethical accurately 777 accurately Schultz\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort will se\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
